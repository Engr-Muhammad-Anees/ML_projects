# -*- coding: utf-8 -*-
"""spam-text-classifier(LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M85AY3VIjoPpOD8RN9bmbxIn4YOrBT7t
"""

import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from keras.layers import Dense, LSTM, Dropout, Input, Embedding, Activation
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping

data = pd.read_csv('/content/archive (6).zip')
data.head()

import matplotlib.pyplot as plt
sns.countplot(data.Label)
plt.xlabel("Label")
plt.title("the number of spam and ham text ")

x = data.Text
y = data.Label
encoder = LabelEncoder()
y = encoder.fit_transform(y)
y = y.reshape(-1, 1)
y

x_train, x_test, y_train, y_test=train_test_split(x,y, test_size=0.20)

max_words= 1000
max_len = 150
tok = Tokenizer(num_words = max_words)
tok.fit_on_texts(x_train)
sequences = tok.texts_to_sequences(x_train)
sequence_matrix = pad_sequences(sequences,maxlen=max_len)
print(sequence_matrix)

def LSTM():
    inputs = Input(name='inputs', shape=[max_len])
    layer = Embedding(max_words, 50, input_length=max_len)(inputs)
    layer = LSTM(64)(layer),
    layer = Dense(256, name='FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.5)(layer)
    layer = Dense(1, name='output_layer')(layer)
    outputs = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=outputs)
    return model

model=LSTM()

model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])

model.fit(sequence_matrix, y_train, epochs=5, batch_size=50)

model.save('/content/drive/MyDrive/Save model IT wing/RNN.text.spam.classifier')

"""**FOR PREDICTION OF THE MODEL ON A SINGLE TEXT:**"""

text_data = x_train.iloc[[459]]
text_data

max_words= 1000
max_len = 150
tok = Tokenizer(num_words = max_words)
tok.fit_on_texts(text_data)
sequences = tok.texts_to_sequences(text_data)
sequences_matrix = pad_sequences(sequences,maxlen=max_len)
print(sequences_matrix)

"""**Model predicted Label:**"""

predict = model.predict(sequences_matrix)
if predict > 0.5:
  print("This is a spam message")
else:
  print("This is a ham message")

"""**Actual text label:**"""

print(data.Label[459])